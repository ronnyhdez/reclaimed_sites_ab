---
title: "Datasets description and validation"
subtitle: "Annotations on data after sampler"
author: Ronny A. Hernandez Mora
execute:
  message: false
  warning: false
format: 
  html:
    theme:
      - flatly
    linkcolor: "#FF5500"
    highlight-style: tango
    toc: true
    toc-title: Table of contents
    toc-location: left
    number-sections: false
    colorlinks: true
    code-fold: true
    code-line-numbers: true
editor: visual
jupyter: python3
editor_options: 
  chunk_output_type: console
---

```{python}
import ee
import json
import os
import sys
import glob
import geemap
import pickle
import seaborn as sns
from pathlib import Path
```

```{python}
ee.Initialize()
```

This file is intended to be a reference of the resulting datasets we have so
far. Before the sampler process, there is a script `create_sampler_assets.py`
which create two assets to be consumed by the `run_sampler.py` script which
will create a group of pkl files with produces Level 2 Vegetation Biophysical 
Products (LEAF-toolbox)

Things to do:

 - List variables from abandoned_wells
 - List variables from reference buffers
 - List variables in pkl files.
 - Define dataset from batches of pkl files
 - Join datasets (unified pkl file with abandoned_wells)
 - Join datasets (unified pkl file with reference_buffers)
 - Number of observations, description of datasets etc.

## Variables from abandoned wells and reference buffers

### Reference buffers



```{python}
sample_wells = ee.FeatureCollection(
    'projects/ee-ronnyale/assets/random_sample_1000_filtered_abandoned_wells')


sample_buffers = ee.FeatureCollection(
  'projects/ee-ronnyale/assets/random_sample_1000_filtered_reference_buffers')
```

```{python}
check_buffers = sample_buffers.limit(1).getInfo()
print(json.dumps(check_buffers, indent = 2))
```

```{python}
check_buffers = sample_buffers.first().getInfo()
properties = check_buffers['properties']

for key, value in properties.items():
              print(f'{key}: {type(value).__name__}')
```

```{python}
print(properties.keys())
```

### Abandoned wells

```{python}
check_wells = sample_wells.limit(1).getInfo()
print(json.dumps(check_wells, indent = 2))
```


```{python}
check_wells = sample_wells.first().getInfo()
properties = check_wells['properties']
print(properties.keys())
```

### Join date variables in buffers for sampler

This is a test that if success needs to be exported as final code to the 
`create_sampler_assets.py` script.


```{python}
def transfer_time_properties(feature):
    matches = ee.List(feature.get('matches'))
    well = ee.Feature(matches.get(0))
    
    # Get year as integer
    year = ee.Number.parse(well.get('rclmtn_d'))  # Parse as number first
    startDate = ee.Date.fromYMD(year, 1, 1).format('dd/MM/yy')
    endDate = ee.Date.fromYMD(2023, 1, 1).format('dd/MM/yy')
    
    return feature \
        .set('system:time_start', startDate) \
        .set('system:time_end', endDate) \
        .set('matches', None)

filter = ee.Filter.equals(
    leftField='wllst__',
    rightField='wllst__'
)

join = ee.Join.saveAll(
    matchesKey='matches',
    outer = True
)

joined = join.apply(
    primary=sample_buffers,
    secondary=sample_wells,
    condition=filter
)

buffers_with_dates = joined.map(transfer_time_properties)
```

```{python}
print(buffers_with_dates.first().getInfo()['properties'])
```

### Temporal code to export FeatureCollection

```{python}
#print("Current working directory:", os.getcwd())
#parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
#print("Parent directory: ", parent_dir)
#sys.path.append(parent_dir)

from gee_helpers.gee_helpers import(
    initialize_gee, get_feature_collection,
    set_dates, set_area, export_if_not_exists
)

# Start the process
initialize_gee()
```

```{python}
export_if_not_exists(
  'projects/ee-ronnyale/assets/reference_buffers_with_dates',
  buffers_with_dates,
 'Export reference buffers with dates for sampler process')
```











## Variables from LEAF-toolbox processed files

```{python}
# Read the LC08 files into one dataframe
pickle_directory = 'data'
pickle_files = glob.glob(f"{pickle_directory}/time_series_LC08_batch_*.pkl")
dataframes = []
for pkl_file in pickle_files:
    with open(pkl_file, 'rb') as file:
        df = pickle.load(file)
        dataframes.append(df)

lc08 = pd.concat(dataframes, ignore_index = True)
```

```{python}
# Check the columns
print(list(lc08.columns))
```

```{python}
lc08.columns
```


```{python}
# Check # observatios per site
grouped_df = lc08.groupby("site").size()
print("Number of observatios per site: ")
print(grouped_df)
```

```{python}
max(grouped_df)
```

```{python}
min(grouped_df)
```

## Joined datasets

```{python}

```
