---
title: "Datasets description and validation"
subtitle: "Annotations on data after sampler"
author: Ronny A. Hernandez Mora
execute:
  message: false
  warning: false
format: 
  html:
    theme:
      - flatly
    linkcolor: "#FF5500"
    highlight-style: tango
    toc: true
    toc-title: Table of contents
    toc-location: left
    number-sections: false
    colorlinks: true
    code-fold: true
    code-line-numbers: true
editor: visual
jupyter: python3
editor_options: 
  chunk_output_type: console
---

```{python}
import ee
import json
import os
import sys
import glob
import geemap
import pickle
import polars as pl
import seaborn as sns
from pathlib import Path
import pandas as pd
```

```{python}
ee.Initialize()
```

This file is intended to be a reference of the resulting datasets we have so
far. Before the sampler process, there is a script `create_sampler_assets.py`
which create two assets to be consumed by the `run_sampler.py` script which
will create a group of pkl files with produces Level 2 Vegetation Biophysical 
Products (LEAF-toolbox)

Things to do:

 - Check asset id
 - List variables from abandoned_wells
 - List variables from reference buffers
 - List variables in pkl files.
 - Define dataset from batches of pkl files
 - Join datasets (unified pkl file with abandoned_wells)
 - Join datasets (unified pkl file with reference_buffers)
 - Number of observations, description of datasets etc.

# Assets ids

Given all the processing steps and the resulting datasets, it is
necessary to validate that the abandoned wells, reference buffers
and resulting outputs from the LEAFtoolbox sampler contain the
same exact observations. 

The main dataset is the filtered abandoned wells which are 1000
randomly selected sites with a unique ID. Then each of these sites
has a reference buffer. And both of them have the results from the
sampler. Each result should contain the same ID which identify
the observations.

```{python}
# Read the LC08 files into one dataframe
pickle_directory = 'data'
pickle_files = glob.glob(f"{pickle_directory}/time_series_LC08_batch_*.pkl")
dataframes = []
for pkl_file in pickle_files:
    with open(pkl_file, 'rb') as file:
        df = pickle.load(file)
        dataframes.append(df)

lc08 = pd.concat(dataframes, ignore_index = True)
```

```{python}
# Assets to compare
abandoned_wells = ee.FeatureCollection(
    'projects/ee-ronnyale/assets/random_sample_1000_filtered_abandoned_wells')
referece_buffers = ee.FeatureCollection(
    'projects/ee-ronnyale/assets/random_sample_1000_filtered_reference_buffers')
filtered_polygons = ee.FeatureCollection(
    'projects/ee-ronnyale/assets/random_sample_1000_filtered_polygons')

# Function to compare IDs
def compare_wllst_values(ee_collections, pandas_df):
    # Function to get sorted list of unique wllst__ values from EE collection
    def get_distinct_ee_values(collection):
        return collection.aggregate_array('wllst__').distinct().sort().getInfo()
    
    # Get wllst__ values from all sources
    values_by_source = {}
    
    # Get values from EE collections
    for name, collection in ee_collections.items():
        values_by_source[name] = set(get_distinct_ee_values(collection))
        print(f"{name} has {len(values_by_source[name])} unique wllst__ values")
    
    # Get values from pandas DataFrame using 'site' column
    pandas_values = set(pandas_df['site'].unique())
    values_by_source['pandas_lc08'] = pandas_values
    print(f"pandas_lc08 has {len(pandas_values)} unique site values")
    
    # Compare all sources with each other
    sources = list(values_by_source.keys())
    for i in range(len(sources)):
        for j in range(i + 1, len(sources)):
            name1 = sources[i]
            name2 = sources[j]
            values1 = values_by_source[name1]
            values2 = values_by_source[name2]
            
            # Find differences
            only_in_1 = values1 - values2
            only_in_2 = values2 - values1
            
            print(f"\nComparing {name1} with {name2}:")
            if only_in_1 or only_in_2:
                if only_in_1:
                    print(f"Values only in {name1}: {sorted(only_in_1)}")
                if only_in_2:
                    print(f"Values only in {name2}: {sorted(only_in_2)}")
            else:
                print(f"They have exactly the same values")

# Create dictionary of EE collections
ee_collections = {
    'abandoned_wells': abandoned_wells,
    #'abandoned_wells_date': abandoned_wells_date,
    'reference_buffers': referece_buffers,
    'filtered_polygons': filtered_polygons
}

# Run the comparison
compare_wllst_values(ee_collections, lc08)
```

# Variables from abandoned wells and reference buffers

To understand what each of the data products contains, this section
presents each of the properties/variables.

## Reference buffers

```{python}
sample_wells = ee.FeatureCollection(
    'projects/ee-ronnyale/assets/random_sample_1000_filtered_abandoned_wells_date_formatted')


sample_buffers = ee.FeatureCollection(
  'projects/ee-ronnyale/assets/random_sample_1000_filtered_reference_buffers_date_formatted')
```

Properties contained in the reference buffers FeatureCollection:

```{python}
check_buffers = sample_buffers.first().getInfo()
properties = check_buffers['properties']

for key, value in properties.items():
              print(f'{key}: {type(value).__name__}')
# print(properties.keys())
```

## Abandoned wells

```{python}
check_wells = sample_wells.limit(1).getInfo()
#print(json.dumps(check_wells, indent = 2))
```

Properties contained in the filtered abandoned wells polygons
FeatureCollection:

```{python}
check_wells = sample_wells.first().getInfo()
properties = check_wells['properties']

for key, value in properties.items():
          print(f'{key}: {type(value).__name__}')
#print(properties.keys())
```

### Export file

To be read locally. There are going to be 1 geojson file read in two ways:

 - `pl.read_json` to get data to be analyzed
 - `gpd.read_file` to read the polygons and plot them

One is going to be using polars for faster data manipulation and analysis
for the "tabular" data. The spatial data will be read with geopandas to
get the GeoJSON polygons for some visualizations.

```{python}
wells_task = ee.batch.Export.table.toDrive(
    collection=sample_wells,
    description='wells_export',
    fileFormat='GeoJSON'
)
wells_task.start()

# Export buffers
buffers_task = ee.batch.Export.table.toDrive(
    collection=sample_buffers,
    description='buffers_export',
    fileFormat='GeoJSON'
)
buffers_task.start()
```

```{python}
wells_df = pl.read_json('wells_export.geojson')
buffers_df = pl.read_json('buffers_export.geojson')

# For spatial operations/visualization
# wells_gdf = gpd.read_file('wells_export.geojson')
# buffers_gdf = gpd.read_file('buffers_export.geojson')
```

## Abandoned wells characteristics

```{python}
wells_df.schema
```

```{python}
wells_df.head()
```

```{python}
wells_df.shape
```

```{python}
wells_df.columns
```


## Variables from LEAF-toolbox processed files

The following are the columns derive from the landsat 8 product for the
abandoned well polygons.

```{python}
lc08.columns
```

The resulting observations of the LEAF-toolbox can be grouped by
their site ID, which is the same as the well site ID

```{python}
# Check # observatios per site
grouped_df = lc08.groupby("site").size()
print("Number of observatios per site: ")
print(grouped_df)
```

```{python}
max(grouped_df)
```

```{python}
min(grouped_df)
```


