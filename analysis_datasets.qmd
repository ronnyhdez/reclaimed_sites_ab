---
title: "Datasets description and validation"
subtitle: "Annotations on data after sampler"
author: Ronny A. Hernandez Mora
execute:
  message: false
  warning: false
format: 
  html:
    theme:
      - flatly
    linkcolor: "#FF5500"
    highlight-style: tango
    toc: true
    toc-title: Table of contents
    toc-location: left
    number-sections: false
    colorlinks: true
    code-fold: true
    code-line-numbers: true
editor: visual
jupyter: python3
editor_options: 
  chunk_output_type: console
---

```{python}
import ee
import json
import os
import sys
import glob
import geemap
import pickle
import seaborn as sns
from pathlib import Path
import pandas as pd
```

```{python}
ee.Initialize()
```

This file is intended to be a reference of the resulting datasets we have so
far. Before the sampler process, there is a script `create_sampler_assets.py`
which create two assets to be consumed by the `run_sampler.py` script which
will create a group of pkl files with produces Level 2 Vegetation Biophysical 
Products (LEAF-toolbox)

Things to do:

 - List variables from abandoned_wells
 - List variables from reference buffers
 - List variables in pkl files.
 - Define dataset from batches of pkl files
 - Join datasets (unified pkl file with abandoned_wells)
 - Join datasets (unified pkl file with reference_buffers)
 - Number of observations, description of datasets etc.

## Variables from abandoned wells and reference buffers

### Reference buffers

```{python}
sample_wells = ee.FeatureCollection(
    'projects/ee-ronnyale/assets/random_sample_1000_filtered_abandoned_wells')


sample_buffers = ee.FeatureCollection(
  'projects/ee-ronnyale/assets/random_sample_1000_filtered_reference_buffers')
```

```{python}
check_buffers = sample_buffers.limit(1).getInfo()
print(json.dumps(check_buffers, indent = 2))
```

```{python}
check_buffers = sample_buffers.first().getInfo()
properties = check_buffers['properties']

for key, value in properties.items():
              print(f'{key}: {type(value).__name__}')
```

```{python}
print(properties.keys())
```

### Abandoned wells

```{python}
check_wells = sample_wells.limit(1).getInfo()
print(json.dumps(check_wells, indent = 2))
```


```{python}
check_wells = sample_wells.first().getInfo()
properties = check_wells['properties']
print(properties.keys())
```

### Join date variables in buffers for sampler

This is a test that if success needs to be exported as final code to the 
`create_sampler_assets.py` script.


```{python}
def transfer_time_properties(feature):
    matches = ee.List(feature.get('matches'))
    well = ee.Feature(matches.get(0))
    
    # Get year as integer
    year = ee.Number.parse(well.get('rclmtn_d'))  # Parse as number first
    startDate = ee.Date.fromYMD(year, 1, 1).format('dd/MM/yy')
    endDate = ee.Date.fromYMD(2023, 1, 1).format('dd/MM/yy')
    
    return feature \
        .set('system:time_start', startDate) \
        .set('system:time_end', endDate) \
        .set('matches', None)

filter = ee.Filter.equals(
    leftField='wllst__',
    rightField='wllst__'
)

join = ee.Join.saveAll(
    matchesKey='matches',
    outer = True
)

joined = join.apply(
    primary=sample_buffers,
    secondary=sample_wells,
    condition=filter
)

buffers_with_dates = joined.map(transfer_time_properties)
```

```{python}
print(buffers_with_dates.first().getInfo()['properties'])
```

### Temporal code to export FeatureCollection

```{python}
#print("Current working directory:", os.getcwd())
#parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
#print("Parent directory: ", parent_dir)
#sys.path.append(parent_dir)

from gee_helpers.gee_helpers import(
    initialize_gee, get_feature_collection,
    set_dates, set_area, export_if_not_exists
)

# Start the process
initialize_gee()
```

```{python}
export_if_not_exists(
  'projects/ee-ronnyale/assets/reference_buffers_with_dates',
  buffers_with_dates,
 'Export reference buffers with dates for sampler process')
```











## Variables from LEAF-toolbox processed files

```{python}
# Read the LC08 files into one dataframe
pickle_directory = 'data'
pickle_files = glob.glob(f"{pickle_directory}/time_series_LC08_batch_*.pkl")
dataframes = []
for pkl_file in pickle_files:
    with open(pkl_file, 'rb') as file:
        df = pickle.load(file)
        dataframes.append(df)

lc08 = pd.concat(dataframes, ignore_index = True)
```

```{python}
# Check the columns
print(list(lc08.columns))
```

```{python}
lc08.columns
```


```{python}
# Check # observatios per site
grouped_df = lc08.groupby("site").size()
print("Number of observatios per site: ")
print(grouped_df)
```

```{python}
max(grouped_df)
```

```{python}
min(grouped_df)
```

## Joined datasets

```{python}

```

# Checking assets ids

```{python}
abandoned_wells = ee.FeatureCollection(
    'projects/ee-ronnyale/assets/random_sample_1000_filtered_abandoned_wells')
referece_buffers = ee.FeatureCollection(
    'projects/ee-ronnyale/assets/random_sample_1000_filtered_reference_buffers')
filtered_polygons = ee.FeatureCollection(
    'projects/ee-ronnyale/assets/random_sample_1000_filtered_polygons')
```

Check ids:

```{python}
def compare_wllst_values(ee_collections, pandas_df):
    # Function to get sorted list of unique wllst__ values from EE collection
    def get_distinct_ee_values(collection):
        return collection.aggregate_array('wllst__').distinct().sort().getInfo()
    
    # Get wllst__ values from all sources
    values_by_source = {}
    
    # Get values from EE collections
    for name, collection in ee_collections.items():
        values_by_source[name] = set(get_distinct_ee_values(collection))
        print(f"{name} has {len(values_by_source[name])} unique wllst__ values")
    
    # Get values from pandas DataFrame using 'site' column
    pandas_values = set(pandas_df['site'].unique())
    values_by_source['pandas_lc08'] = pandas_values
    print(f"pandas_lc08 has {len(pandas_values)} unique site values")
    
    # Compare all sources with each other
    sources = list(values_by_source.keys())
    for i in range(len(sources)):
        for j in range(i + 1, len(sources)):
            name1 = sources[i]
            name2 = sources[j]
            values1 = values_by_source[name1]
            values2 = values_by_source[name2]
            
            # Find differences
            only_in_1 = values1 - values2
            only_in_2 = values2 - values1
            
            print(f"\nComparing {name1} with {name2}:")
            if only_in_1 or only_in_2:
                if only_in_1:
                    print(f"Values only in {name1}: {sorted(only_in_1)}")
                if only_in_2:
                    print(f"Values only in {name2}: {sorted(only_in_2)}")
            else:
                print(f"They have exactly the same values")

# Create dictionary of EE collections
ee_collections = {
    'abandoned_wells': abandoned_wells,
    #'abandoned_wells_date': abandoned_wells_date,
    'reference_buffers': referece_buffers,
    'filtered_polygons': filtered_polygons
}

# Run the comparison
compare_wllst_values(ee_collections, lc08)
```
