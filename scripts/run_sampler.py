"""
Run LEAF toolbox sampler

This runs the LEAF toolbox sampler over the polygons for
the required image collections products. It performs the
following steps:

1. Initializes the GEE API
2. Divides the asset into batches
3. Export each batch as an asset
4. Runs the sampler for each bacth
5. Extracts the results and save them as pkl files

Parameters:

- polygon_collection: The feature collection of polygons from 
  the specified site.
- total_polygons: The total number of polygons in the feature
  collection.
- image_collections: A list of dictionaries containing the 
  image collection names and labels.
- batch_asset_id: The ID for the temporary batch asset in GEE.

Outputs:
- Pickle files: One pickle file per image collection and batch,
  saved in the current directory.

Usage:
- Run the script with the specified site and batch_size.
- The script will automatically handle the processing of 
  the batches and save the results.
- If a batch has already been processed, it will skip that
  batch to avoid duplication.

Author: Ronny A. Hern√°ndez Mora
"""

import os
import sys
import pickle
import time
import ee
import pandas as pd

print("Current working directory:", os.getcwd())

parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
print("Parent directory: ", parent_dir)
sys.path.append(parent_dir)

# Import functions from custom helper module
from gee_helpers.gee_helpers import initialize_gee, get_feature_collection

# PARAMETERS
# This first random_sample_1000_filtered_polygons works due to the transformation
# in the date format created in GEE script create_sampler_asset. This needs
# to be implemented here for the correspondent assets.
# POLYGONS_FEATURE_COLLECTION = 'projects/ee-ronnyale/assets/random_sample_1000_filtered_polygons'

## These are tests. 
# POLYGONS_FEATURE_COLLECTION = 'projects/ee-ronnyale/assets/random_sample_1000_filtered_abandoned_wells'
POLYGONS_FEATURE_COLLECTION = 'projects/ee-ronnyale/assets/random_sample_1000_filtered_reference_buffers_date_formatted'
PROJECT_TO_SAVE_ASSETS = 'projects/ee-ronnyale/assets/'
DATA_OUTPUT_DIR = 'data_buffers/'

initialize_gee()


# Import LEAFtoolbox modules
module_path = os.path.abspath(os.path.join('..'))
print(module_path)
if module_path not in sys.path:
    sys.path.append(module_path)

from leaftoolbox import LEAF
from leaftoolbox import SL2PV0 

# Start the process
batch_size = 20
polygon_collection = get_feature_collection(POLYGONS_FEATURE_COLLECTION)
total_polygons = polygon_collection.size().getInfo()

# Products to be processed
image_collections = [
    {"name": "LANDSAT/LC08/C02/T1_L2", "label": "LC08"},
    {"name": "LANDSAT/LC09/C02/T1_L2", "label": "LC09"},
    # {"name": "COPERNICUS/S2_SR_HARMONIZED", "label": "S2"}        
]

for collection in image_collections:
    image_collection_name = collection["name"]
    label = collection["label"]

    # Process each batch
    # for start_index in range(0, 20, 20):
    for start_index in range(0, total_polygons, batch_size):
        pickle_filename = f'{DATA_OUTPUT_DIR}time_series_{label}_batch_{start_index}.pkl'

        # Check if pkl file for the batch already exists
        if os.path.exists(pickle_filename):
            print(f'Batch {start_index} for {label} already processed and saved. Skipping...')
            continue
        batch = polygon_collection.toList(batch_size, start_index)
        batch_fc = ee.FeatureCollection(batch)
        batch_asset_id = f'{PROJECT_TO_SAVE_ASSETS}_temp_batch_{label}_{start_index}'
        task = ee.batch.Export.table.toAsset(
            collection = batch_fc,
            description = f'export_batch_{label}_{start_index}',
            assetId = batch_asset_id
        )
        print(f'Exporting batch {start_index} for {label} to GEE')
        task.start()

        # Avoid running if asset is not ready yet
        while task.status()['state'] in ['READY', 'RUNNING']:
            time.sleep(10)

        start_time = time.time()
        if label in ["LC08", "LC09"]:
          sites_dictionary = LEAF.sampleSites(
              [batch_asset_id],
              imageCollectionName = image_collection_name,
              algorithm = SL2PV0,
              variableName = "Surface_Reflectance",
              maxCloudcover = 90,
              outputScaleSize = 30,
              inputScaleSize = 30,
              bufferSpatialSize = 0,
              numPixels = 100
          )
        elif label == "S2":
          sites_dictionary = LEAF.sampleSites(
              [batch_asset_id],
              imageCollectionName = image_collection_name,
              algorithm = SL2PV0,
              variableName = "Surface_Reflectance",
              maxCloudcover = 90,
              outputScaleSize = 20,
              inputScaleSize = 20,
              bufferSpatialSize = 0,
              bufferTemporalSize = ['2020-01-01','2020-12-01'],
              numPixels = 100
          )
        end_time = time.time()
        execution_time = end_time - start_time
        print(f'Execution time for batch {start_index} with {label}: {execution_time} seconds')

        # Extract and process results
        outer_key = list(sites_dictionary.keys())[0]
        first_item = sites_dictionary[outer_key]
        batch_results = []

        for item in range(len(first_item)):
            df = first_item[item]['leaftoolbox.SL2PV0']
            df['site'] = first_item[item]['feature']['wllst__']
            batch_results.append(df)

        # Combine batch results
        combined_df = pd.concat(batch_results, ignore_index = True)
        with open(pickle_filename, 'wb') as file:
            pickle.dump(combined_df, file)
        
        print(f'Batch {start_index} for {label} saved to {pickle_filename}')
